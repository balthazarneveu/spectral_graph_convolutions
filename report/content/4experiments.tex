\section{Experiments}

\subsection{Proposed Modifications}
We studied various aspects of the methodology, including:
\begin{enumerate}
    \item The relevance of dimensionality reduction applied to features before training the neural networks.
    \item The approach of choosing a graph-based classification approach rather than a simple dense network operating on the vectorized feature vectors.
\end{enumerate}
As a result, we explored different techniques for dimensionality reduction and various classification models. We run the following experiments to address these questions.

Initially, we trained the models using the original features without any dimensionality reduction (1). Subsequently, we trained them with a reduced number of features achieved through recursive feature elimination (2), mirroring the approach detailed in the referenced paper. Finally, we applied an autoencoder to reduce feature dimensionality (3).

We implemented three distinct models:
\begin{enumerate}
    \item A simple Dense Neural Network consisting of linear layers and ReLU activations.
    \item A Graph Convolutional Network (GCN) as outlined in \cite{kipf_semi-supervised_2017}, incorporating the \textit{renormalization trick}.
    \item A neural network utilizing Chebyshev polynomials with an order of $K=3$, in accordance with the methodology presented in the referenced paper \cite{Parisot17}.
\end{enumerate}


\subsection{Dimensionality reduction by Recursive Feature Elimination with a Ridge classifier}

A Ridge classifier is a standard linear classifier with an added regularization term known as the Ridge (L2) penalty.

The optimization problem of the ridge classifier is to find the coefficients $\beta$ that minimize the following objective function:
$$
\min_{\beta} \norm{y - X\beta}_2^2 + \alpha \norm{\beta}_2^2
$$
where $X$ is the design matrix, here the vectorized connectivity matrix, $y$ is the corresponding target value, $\alpha$ is the regularization parameter, and $\norm{.}_2$ denotes the Euclidean norm. The ridge classifier converts the targets values into $\{-1, 1\}$ and then treats the problem as a regression task. The predicted class corresponds to the sign of the regressor's prediction, which is
$$
\hat{y} = sign(X\hat{\beta})
$$
where $sign$ is a function that return $1$ if the input is positive, $-1$ if the input is negative, and $0$ if the input is zero.
% The Ridge penalty term, $\alpha \lVert w \rVert_2^2$, penalizes large values of the weights, encouraging a more generalizable model and helping to mitigate overfitting.

The recursive feature elimination (RFE) is employed to iteratively select features by progressively considering smaller sets of features. Initially, the estimator is trained on the complete set of features, and the importance of each feature $i$ is determined by evaluating the weights assigned to the Ridge estimator $\hat{\beta}_i$. Subsequently, the least important features are pruned from the current set and this process is repeated recursively on the pruned set until the desired number of features to select is reached.
% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/rfe.png}
%     \caption{Relative importance of features for recursive elimination}
%     \label{fig:rfe}
%     \Description{}
% \end{figure*}

\subsection{Dimensionality reduction by Autoencoder}

\quad Autoencoders are a type of neural network architecture widely employed for dimensionality reduction in machine learning applications. When applied to feature selection, autoencoders aim to learn a compressed representation of the input data, effectively and automatically capturing its essential features.

We tested these autoencoders against Recursive Feature Elimination (RFE) and using all input features. We implemented simple autoencoders, the encoder and the decoder contain one linear layer each, and the bottleneck has the same dimension as the number of features to select used in the RFE, finally, we chose a tanh function as an activation function between these layers.

\begin{figure}[h!]
    \includegraphics[width=0.5\textwidth]{figures/autoencoders_results.png}
    \caption{Comparison of dimensionality reduction methods - including using autoencoders.}
    \label{fig:autoencoders}
    \Description{}
\end{figure}
 
The results obtained are reported in the figure \ref{fig:autoencoders}. While autoencoders provide close accuracies, they fall slightly short of the performance achieved by the other methods. This outcome suggests that, in the context of disease prediction classifier, RFE and utilizing the complete set of input features may yield slightly better results. Understanding the trade-offs between these techniques is crucial in selecting an appropriate dimensionality reduction strategy that aligns with the specific characteristics and requirements of your dataset and prediction task.


