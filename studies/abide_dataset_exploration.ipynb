{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medigraph.data.abide import AbideData\n",
    "import numpy as np\n",
    "from nilearn import plotting\n",
    "from medigraph.model.gcn import GCN, SparseGCN, ChebGCN\n",
    "from medigraph.model.baseline import DenseNN\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from medigraph.data.preprocess import sanitize_data, visual_sanity_check_input, whiten\n",
    "from medigraph.data.experiences_data import get_training_dict_exp1, get_training_dict_exp2\n",
    "from medigraph.train import training_loop, train, plot_learning_curves, compute_metrics\n",
    "from medigraph.data.properties import INPUTS, LABELS, TRAIN_MASK, VAL_MASK, TEST_MASK, ADJ\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = AbideData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check connectivy matrix for a single patient\n",
    "- 111x111 matrices\n",
    "- We'll retrieve the $6216=\\frac{111*(111+1)}{2}$ raw coefficients from the upper triangular matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the connectivity matrix for the first subject\n",
    "idx = 0\n",
    "mat = dat.get_connectivity_matrix(idx)\n",
    "plotting.plot_matrix(\n",
    "    mat,\n",
    "    figure=(6, 6),\n",
    "    vmax=1,\n",
    "    vmin=0,\n",
    "    title=f\"Patient {idx} connectivity matrix {mat.shape}\"\n",
    ")\n",
    "feature_vector_input = dat.get_connectivity_features(idx)\n",
    "print(f\"input feature vector shape: {feature_vector_input.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build adjacency, features matrix and classification labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Build adjacency matrix and input feature vectors\n",
    "data_dict = dat.get_training_data(dimension_reduction=\"raw_inputs\")\n",
    "inp_np, lab_np, adj_np = data_dict[INPUTS], data_dict[LABELS], data_dict[ADJ]\n",
    "print(f\"Adjacency matrix : {adj_np.shape} [VxV]\")\n",
    "print(f\"Labels {lab_np.shape} : [V]\")\n",
    "print(f\"Input feature vector {inp_np.shape} : [VxF]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Load data to GPU\n",
    "labels_np = dat.get_labels()\n",
    "adj = torch.tensor(adj_np, dtype=torch.float32).to(device)\n",
    "inp_raw = torch.tensor(inp_np, dtype=torch.float32).to(device)  # [V=871,  F6216]\n",
    "lab = torch.tensor(labels_np, dtype=torch.float32).to(device)  # for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Sanitize and whiten data\n",
    "clean_inp = sanitize_data(inp_raw)\n",
    "inp = whiten(clean_inp)\n",
    "inp.shape, adj.shape, lab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Visalization of sanity check\n",
    "visual_sanity_check_input(inp_raw)\n",
    "visual_sanity_check_input(clean_inp)\n",
    "visual_sanity_check_input(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % sanity check on graph adjacency matrix\n",
    "model = GCN(inp.shape[1], adj, hdim=64)\n",
    "plotting.plot_matrix(\n",
    "    model.adj.detach().cpu().numpy(),\n",
    "    figure=(6, 6),\n",
    "    vmax=0.005,\n",
    "    vmin=0,\n",
    "    title=f\"Graph normalized adjacency matrix {mat.shape}\"\n",
    ")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = {\n",
    "    INPUTS: inp,\n",
    "    LABELS: lab\n",
    "}\n",
    "metric_dict = {}\n",
    "for model_name in [\"Dense\", \"GCN\"]:\n",
    "    if model_name == \"GCN\":\n",
    "        model = GCN(inp.shape[1], adj, hdim=64)\n",
    "    else:\n",
    "        model = DenseNN(inp.shape[1], hdim=64)\n",
    "    model.to(device)\n",
    "\n",
    "    model, metrics = training_loop(model, training_data, device, n_epochs=1000)\n",
    "    metric_dict[model_name] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metric_dict: dict):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 6))\n",
    "    for model_name, metric in metric_dict.items():\n",
    "        print(metric.keys())\n",
    "        axs[0].plot(metric[\"training_losses\"], label=model_name)\n",
    "        axs[1].plot(metric[\"training_accuracies\"], label=f\"{model_name} accuracy\")\n",
    "    for ax in axs:\n",
    "        ax.legend()\n",
    "        ax.grid()\n",
    "    axs[0].set_title(\"Training loss (Binary Cross Entropy)\")\n",
    "    axs[1].set_title(\"Accuracy\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_metrics(metric_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exp 1 : Train classifiers without reduction of features\n",
    "\n",
    "Here we trained the classifier on all the features (using the 'sanitizer made by Balthazar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dict_exp1 = get_training_dict_exp1(dat, nb_train=700, nb_val=100, override=False)\n",
    "# put to tensor and to device\n",
    "training_dict_exp1[TRAIN_MASK] = torch.LongTensor(training_dict_exp1[TRAIN_MASK]).to(device)\n",
    "training_dict_exp1[VAL_MASK] = torch.LongTensor(training_dict_exp1[VAL_MASK]).to(device)\n",
    "training_dict_exp1[TEST_MASK] = torch.LongTensor(training_dict_exp1[TEST_MASK]).to(device)\n",
    "training_dict_exp1[INPUTS] = torch.tensor(training_dict_exp1[INPUTS], dtype=torch.float32).to(device)\n",
    "training_dict_exp1[LABELS] = torch.tensor(training_dict_exp1[LABELS], dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "\n",
    "in_channels_exp1 = training_dict_exp1[INPUTS].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand and visualize the feature (reflection to add to the report ?)\n",
    "\n",
    "plot the correlation matrix, visualize with PCA or with UMAP ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline : DenseNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DenseNN_exp1 = DenseNN(in_channels_exp1,\n",
    "                       hdim=64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_dict_exp1[LABELS].ndim > 1:\n",
    "    training_dict_exp1[LABELS] = training_dict_exp1[LABELS].squeeze(1)\n",
    "DenseNN_exp1, DenseNN_exp1_train_log, DenseNN_exp1_val_log = train(DenseNN_exp1, \n",
    "                                                                   training_dict_exp1,\n",
    "                                                                   nEpochs=200,\n",
    "                                                                   optimizer_params={'lr': 0.001, \n",
    "                                                                                     'weight_decay': 0.05})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(DenseNN_exp1_train_log, DenseNN_exp1_val_log, title=\"Training of DenseNN_exp1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing \n",
    "loss_test, acc_test = compute_metrics(DenseNN_exp1, training_dict_exp1)\n",
    "print(f\"Test loss: {loss_test.item():.4f}, Test accuracy: {acc_test.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_dict_exp1[LABELS].ndim == 1:\n",
    "    training_dict_exp1[LABELS] = training_dict_exp1[LABELS].unsqueeze(1)\n",
    "\n",
    "GCN_exp1 = SparseGCN(in_channels_exp1, \n",
    "                  nhid=[256, 128], \n",
    "                  nclass=1, \n",
    "                  adjacency=training_dict_exp1[ADJ], \n",
    "                  proba_dropout=0.6,\n",
    "                  device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCN_exp1, GCN_exp1_train_log, GCN_exp1_val_log = train(GCN_exp1, training_dict_exp1, \n",
    "                                              nEpochs=200, \n",
    "                                              optimizer_params={'lr': 0.01, 'weight_decay': 0.05})\n",
    "# torch.save(GCN_exp1.state_dict(), \"__GCN_exp1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(GCN_exp1_train_log, GCN_exp1_val_log, title=\"Training of GCN_exp1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing \n",
    "loss_test, acc_test = compute_metrics(GCN_exp1, training_dict_exp1)\n",
    "print(f\"Test loss: {loss_test.item():.4f}, Test accuracy: {acc_test.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChebGCN on exp1\n",
    "TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exp 2 : Train classifiers with selected feat\n",
    "\n",
    "Here, we train the same network with features reduced by a ridge classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dict_exp2 = get_training_dict_exp2(dat, nb_train=700, nb_val=100, override=False)\n",
    "# put to tensor and to device\n",
    "training_dict_exp2[TRAIN_MASK] = torch.LongTensor(training_dict_exp2[TRAIN_MASK]).to(device)\n",
    "training_dict_exp2[VAL_MASK] = torch.LongTensor(training_dict_exp2[VAL_MASK]).to(device)\n",
    "training_dict_exp2[TEST_MASK] = torch.LongTensor(training_dict_exp2[TEST_MASK]).to(device)\n",
    "training_dict_exp2[INPUTS] = torch.tensor(training_dict_exp2[INPUTS], dtype=torch.float32).to(device)\n",
    "# training_dict_exp2[LABELS] = torch.tensor(training_dict_exp2[LABELS], dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "training_dict_exp2[LABELS] = torch.tensor(training_dict_exp2[LABELS], dtype=torch.float32).to(device)\n",
    "\n",
    "in_channels_exp2 = training_dict_exp2[INPUTS].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand and visualize the data to compare with the precedent experience \n",
    "\n",
    "Correlation matrix, PCA, UMAP ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA : TO DO\n",
    "\n",
    "# selected_feat = get_training_dict_exp2[INPUTS]\n",
    "# print(selected_feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline : DenseNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DenseNN_exp2 = DenseNN(in_channels_exp2,\n",
    "                       hdim=64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_dict_exp2[LABELS].ndim > 1:\n",
    "    training_dict_exp2[LABELS] = training_dict_exp2[LABELS].squeeze(1)\n",
    "DenseNN_exp2, DenseNN_exp2_train_log, DenseNN_exp2_val_log = train(DenseNN_exp2, \n",
    "                                                                   training_dict_exp2,\n",
    "                                                                   nEpochs=200,\n",
    "                                                                   optimizer_params={'lr': 0.001, \n",
    "                                                                                     'weight_decay': 0.05})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(DenseNN_exp2_train_log, DenseNN_exp2_val_log, title=\"Training of DenseNN_exp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing \n",
    "loss_test, acc_test = compute_metrics(DenseNN_exp2, training_dict_exp2)\n",
    "print(f\"Test loss: {loss_test.item():.4f}, Test accuracy: {acc_test.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_dict_exp2[LABELS].ndim == 1:\n",
    "    training_dict_exp2[LABELS] = training_dict_exp2[LABELS].unsqueeze(1)\n",
    "\n",
    "GCN_exp2 = SparseGCN(in_channels_exp2, \n",
    "                  nhid=[128, 64], \n",
    "                  nclass=1, \n",
    "                  adjacency=training_dict_exp2[ADJ], \n",
    "                  proba_dropout=0.3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing  before training to see if the model get better\n",
    "loss_test, acc_test = compute_metrics(GCN_exp2, training_dict_exp2)\n",
    "print(f\"Test Before Training \\n loss: {loss_test.item():.4f}, Test accuracy: {acc_test.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_dict_exp2[LABELS] = training_dict_exp2[LABELS].squeeze(1)\n",
    "GCN_exp2, GCN_exp2_train_log, GCN_exp2_val_log = train(GCN_exp2, training_dict_exp2, \n",
    "                                              nEpochs=200, \n",
    "                                              optimizer_params={'lr': 0.001, 'weight_decay': 0.005})\n",
    "# torch.save(trained_model_GCN.state_dict(), \"__trained_model_GCN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(GCN_exp2_train_log, GCN_exp2_val_log, title=\"Training of GCN_exp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(GCN_exp2_val_log[\"accuracies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing after training\n",
    "loss_test, acc_test = compute_metrics(GCN_exp2, training_dict_exp2)\n",
    "print(f\"Test loss: {loss_test.item():.4f}, Test accuracy: {acc_test.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChebConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChebGCN_exp2 = ChebGCN(in_channels_exp2, out_features=1, K=3,\n",
    "                       adjacency=training_dict_exp2[ADJ],\n",
    "                       proba_dropout=0.3, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChebGCN_exp2, ChebGCN_exp2_train_log, ChebGCN_exp2_val_log = train(ChebGCN_exp2, training_dict_exp2,\n",
    "                                            nEpochs=200,\n",
    "                                            optimizer_params={'lr': 0.005, 'weight_decay': 0.005})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(ChebGCN_exp2_train_log, ChebGCN_exp2_val_log, title=\"Training of ChebGCN_exp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "loss_test, acc_test = test(ChebGCN_exp2, training_dict_exp2)\n",
    "print(f\"Test loss: {loss_test.item():.4f}, Test accuracy: {acc_test.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
